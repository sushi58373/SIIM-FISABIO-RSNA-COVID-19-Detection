{"cells":[{"cell_type":"markdown","metadata":{},"source":["# [TRAIN] 2 classifier\n","\n","[[Notebook] Reference Notebook for 2 classifier](https://www.kaggle.com/h053473666/siim-covid19-efnb7-train-fold0-5-2class?select=train.csv)\n","\n","what is 2 classifier?  \n","either ***object(box)*** or ***Not***   \n","image level 에서 none 과 그렇지 않은 것을 구분하는 classifier.  \n","왜 그렇게 하는가? 조사  \n","\n","1. efficientnet b7 kfold traininig on TPU\n","2. efficientnetV2_large [[Notebook] Reference Notebook for using EffNetV2](https://www.kaggle.com/sreevishnudamodaran/siim-effnetv2-keras-study-train-tpu-cv0-805/notebook)\n","\n","-> 둘 다 인터넷 되므로 패키지 설치할 필요 없다.\n","\n","추후에 이미지 resize 생성시, resolution 상승시켜 학습도 해야겠다.   \n","\n","----\n","\n","### [[Discussion] host - annotation](https://www.kaggle.com/c/siim-covid19-detection/discussion/246597)  \n","\n","테스트 데이터 세트에 대해서는 label 업데이트. but, train label 은 그대로 유지.  \n","\n","test data label 은 수정되었으나, train data label 은 시간 제약으로 동일하게 유지되고 업데이트 되지 않았다.  \n","\n","→ 따라서 연구에 속한 이미지가 2개 이상인 경우, bounding boxes 가 있는 이미지만 사용하는 것을 추천한다.  \n","\n","**즉, 데이터셋을 아예 새롭게 구성한다.**\n","\n","위와 같이 dataset 중 study_level 마다 이미지가 2개이상인 데이터의 경우, 같은 이미지임에도 box가 있고 없는 이미지들이 있다.  \n","test set 은 추후에 label annotating 보정이 되었기 때문에, train data 에서 제외시켜야 한다.   \n","또한 이러한 이미지들은 none 여부에 영향을 미치므로 detection 만이 아니라 2 class 도 다시 학습시켜준다.  \n","study_level 도 다시 학습 시킬 예정이긴 하나, study_level 은 study 단위이므로 큰 의미는 없을 것이다.\n","그렇게 다시 efnb7 부터 학습을 다시 시행해본다.  "]},{"cell_type":"markdown","metadata":{},"source":["### Dataset GCS path \n","Kaggle에서 Dataset 생성시 바로 download가 안될때 보면 gcs path 가 없다고 나온다.  \n","아마 이전에 계속 오류가 뜬 부분이 여기가 아닌가 싶다.  \n","이전에 private 형태로 사용하기 위한 절차를 수행했는데, 그 과정에서 문제가 생긴게 아닌가 싶다.  \n","Dataset 을 재생성했고, gcs path 가 생성된 것을 확인 후 run -> ok"]},{"cell_type":"markdown","metadata":{},"source":["### TPU for Private Datasets\n","[[Discussion] TPU for Private Datasets](https://www.kaggle.com/product-feedback/163416#1298583)  \n","[[Article] How to use TPU](https://www.kaggle.com/docs/tpu)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:30.144102Z","iopub.status.busy":"2021-07-31T03:31:30.143652Z","iopub.status.idle":"2021-07-31T03:31:30.149110Z","shell.execute_reply":"2021-07-31T03:31:30.147812Z","shell.execute_reply.started":"2021-07-31T03:31:30.144013Z"},"trusted":true},"outputs":[],"source":["# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# user_credential = user_secrets.get_gcloud_credential()\n","# user_secrets.set_tensorflow_credential(user_credential)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:30.151795Z","iopub.status.busy":"2021-07-31T03:31:30.151218Z","iopub.status.idle":"2021-07-31T03:31:49.203384Z","shell.execute_reply":"2021-07-31T03:31:49.202274Z","shell.execute_reply.started":"2021-07-31T03:31:30.151695Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","TF version: 2.4.1\n","Hub version: 0.12.0\n","Physical devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"]}],"source":["!pip install efficientnet -q\n","\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import efficientnet.tfkeras as efn\n","# use TPU by kaggle\n","from kaggle_datasets import KaggleDatasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GroupKFold\n","from sklearn.metrics import roc_auc_score\n","import tensorflow as tf\n","import shutil\n","\n","from glob import glob\n","import itertools\n","import random\n","import gc\n","import math\n","import pprint\n","import seaborn as sns\n","import tensorflow_hub as tfhub\n","import tensorflow.keras.backend as K\n","\n","import albumentations\n","from PIL import Image, ImageOps, ImageEnhance\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","from albumentations.augmentations import functional as F\n","\n","print('TF version:', tf.__version__)\n","print('Hub version:', tfhub.__version__)\n","print('Physical devices:', tf.config.list_physical_devices())"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.205930Z","iopub.status.busy":"2021-07-31T03:31:49.205611Z","iopub.status.idle":"2021-07-31T03:31:49.213918Z","shell.execute_reply":"2021-07-31T03:31:49.212737Z","shell.execute_reply.started":"2021-07-31T03:31:49.205902Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Use model Version : V2\n"]}],"source":["# Config for EfficientNetV2\n","\n","v1 = False\n","v2 = True\n","\n","def seed_everything(SEED):\n","    os.environ['PYTHONHASHSEED'] = str(SEED)\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    tf.random.set_seed(SEED)\n","    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n","\n","print(f\"Use model Version : {'V1' if v1 else 'V2'}\")\n","seed_everything(42)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.216320Z","iopub.status.busy":"2021-07-31T03:31:49.215961Z","iopub.status.idle":"2021-07-31T03:31:49.226773Z","shell.execute_reply":"2021-07-31T03:31:49.225583Z","shell.execute_reply.started":"2021-07-31T03:31:49.216288Z"},"trusted":true},"outputs":[],"source":["# original Notebook (https://www.kaggle.com/sreevishnudamodaran/siim-effnetv2-keras-study-train-tpu-cv0-805)\n","# def get_mat : transform matrix\n","# def transform : augmentation"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.228955Z","iopub.status.busy":"2021-07-31T03:31:49.228556Z","iopub.status.idle":"2021-07-31T03:31:49.253889Z","shell.execute_reply":"2021-07-31T03:31:49.252382Z","shell.execute_reply.started":"2021-07-31T03:31:49.228924Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Fucntion\n"]}],"source":["SATURATION  = (0.9, 1.1)\n","CONTRAST = (0.9, 1.1)\n","BRIGHTNESS  =  0.1\n","ROTATION    = 10.0\n","SHEAR    = 2.0\n","HZOOM  = 8.0\n","WZOOM  = 4.0\n","HSHIFT = 4.0\n","WSHIFT = 4.0\n","\n","def auto_select_accelerator():\n","    \"\"\"\n","        자동으로 가속기 선택. \n","        조건이 될 때는 TPU. \n","        조건이 안될때는 할당된 가속기 선택.\n","    \"\"\"\n","    # Detect hardware, return appropriate distribution strategy\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        tf.tpu.experimental.initialize_tpu_system(tpu)\n","        strategy = tf.distribute.experimental.TPUStrategy(tpu) # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","        print(\"Running on TPU:\", tpu.master())\n","    except ValueError:\n","        strategy = tf.distribute.get_strategy()\n","    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n","    \n","    return strategy\n","\n","def build_decoder(with_labels=True, target_size=(256, 256), ext='jpg'):\n","    def decode(path):\n","        file_bytes = tf.io.read_file(path)\n","\n","        if ext == 'png':\n","            img = tf.image.decode_png(file_bytes, channels=3)\n","        elif ext in ['jpg', 'jpeg']:\n","            img = tf.image.decode_jpeg(file_bytes, channels=3)\n","        else:\n","            raise ValueError(\"Image extension not supported\")\n","        img = tf.cast(img, tf.float32) / 255.0\n","        img = tf.image.resize(img, target_size)\n","\n","        return img\n","    \n","    def decode_with_labels(path, label):\n","        return decode(path), label\n","    \n","    return decode_with_labels if with_labels else decode\n","\n","def build_augmenter(with_labels=True):\n","    def augment(img):\n","        img = tf.image.random_flip_left_right(img)\n","#        img = tf.image.random_flip_up_down(img)\n","        img = tf.image.random_saturation(img, SATURATION[0], SATURATION[1])\n","        img = tf.image.random_contrast(img, CONTRAST[0], CONTRAST[1])\n","        img = tf.image.random_brightness(img, BRIGHTNESS)\n","        return img\n","    \n","    def augment_with_labels(img, label):\n","        return augment(img), label\n","    \n","    return augment_with_labels if with_labels else augment\n","# 다른 Notebook 에서는 flip_up_down 을 지우고(하긴 위아래가 바뀔 일을 없으니)\n","# saturation, contrast, brightness 를 추가하였음.\n","\n","def build_dataset(paths, labels=None, bsize=128, cache=True,\n","                  decode_fn=None, augment_fn=None, do_mix = True,\n","                  augment=True, repeat=True, shuffle=1024, \n","                  cache_dir=\"\"):\n","    if cache_dir != \"\" and cache is True:\n","        os.makedirs(cache_dir, exist_ok=True)\n","    \n","    if decode_fn is None:\n","        decode_fn = build_decoder(labels is not None)\n","    \n","    if augment_fn is None:\n","        augment_fn = build_augmenter(labels is not None)\n","    \n","    AUTO = tf.data.experimental.AUTOTUNE\n","    slices = paths if labels is None else (paths, labels)\n","    \n","    dset = tf.data.Dataset.from_tensor_slices(slices)\n","    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n","    dset = dset.cache(cache_dir) if cache else dset\n","    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset # augmentation\n","    dset = dset.repeat() if repeat else dset\n","    dset = dset.shuffle(shuffle) if shuffle else dset\n","    dset = dset.batch(bsize).prefetch(AUTO)\n","    dset = dset.map(transform, num_parallel_calls=AUTO) if do_mix else dset # cutmix\n","    return dset\n","\n","print(\"Load Fucntion\")"]},{"cell_type":"markdown","metadata":{},"source":["## Mixed Precision and/or XLA\n","\n","Mixed Precision and XLA are not being used in this notebook but you can experiment using them. Change the following booleans to enable mixed precision and/or XLA on GPU/TPU. By default TPU already uses some mixed precision but we can add more (and it already uses XLA). These allow the GPU/TPU memory to handle larger batch sizes and can speed up the training process. The Nvidia V100 GPU has special Tensor Cores which get utilized when mixed precision is enabled. Unfortunately Kaggle's Nvidia P100 GPU does not have Tensor Cores to receive speed up."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.255794Z","iopub.status.busy":"2021-07-31T03:31:49.255398Z","iopub.status.idle":"2021-07-31T03:31:49.269946Z","shell.execute_reply":"2021-07-31T03:31:49.268757Z","shell.execute_reply.started":"2021-07-31T03:31:49.255760Z"},"trusted":true},"outputs":[],"source":["MIXED_PRECISION = False\n","XLA_ACCELERATE = False\n","\n","if MIXED_PRECISION:\n","    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n","    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n","    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n","    mixed_precision.set_policy(policy)\n","    print('Mixed precision enabled')\n","\n","if XLA_ACCELERATE:\n","    tf.config.optimizer.set_jit(True)\n","    print('Accelerated Linear Algebra enabled')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.272307Z","iopub.status.busy":"2021-07-31T03:31:49.271803Z","iopub.status.idle":"2021-07-31T03:31:49.450302Z","shell.execute_reply":"2021-07-31T03:31:49.449266Z","shell.execute_reply.started":"2021-07-31T03:31:49.272260Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['none', 'box'], dtype='object')\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>boxes</th>\n","      <th>label</th>\n","      <th>StudyInstanceUID</th>\n","      <th>none_or_box</th>\n","      <th>none</th>\n","      <th>box</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000a312787f2_image</td>\n","      <td>[{'x': 789.28836, 'y': 582.43035, 'width': 102...</td>\n","      <td>opacity 1 789.28836 582.43035 1815.94498 2499....</td>\n","      <td>5776db0cec75</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000c3a3f293f_image</td>\n","      <td>NaN</td>\n","      <td>none 1 0 0 1 1</td>\n","      <td>ff0879eb20ed</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0012ff7358bc_image</td>\n","      <td>[{'x': 677.42216, 'y': 197.97662, 'width': 867...</td>\n","      <td>opacity 1 677.42216 197.97662 1545.21983 1197....</td>\n","      <td>9d514ce429a7</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001398f4ff4f_image</td>\n","      <td>[{'x': 2729, 'y': 2181.33331, 'width': 948.000...</td>\n","      <td>opacity 1 2729 2181.33331 3677.00012 2785.33331</td>\n","      <td>28dddc8559b2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>001bd15d1891_image</td>\n","      <td>[{'x': 623.23328, 'y': 1050, 'width': 714, 'he...</td>\n","      <td>opacity 1 623.23328 1050 1337.23328 2156 opaci...</td>\n","      <td>dfd9fdd85a3e</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   id                                              boxes  \\\n","0  000a312787f2_image  [{'x': 789.28836, 'y': 582.43035, 'width': 102...   \n","1  000c3a3f293f_image                                                NaN   \n","2  0012ff7358bc_image  [{'x': 677.42216, 'y': 197.97662, 'width': 867...   \n","3  001398f4ff4f_image  [{'x': 2729, 'y': 2181.33331, 'width': 948.000...   \n","4  001bd15d1891_image  [{'x': 623.23328, 'y': 1050, 'width': 714, 'he...   \n","\n","                                               label StudyInstanceUID  \\\n","0  opacity 1 789.28836 582.43035 1815.94498 2499....     5776db0cec75   \n","1                                     none 1 0 0 1 1     ff0879eb20ed   \n","2  opacity 1 677.42216 197.97662 1545.21983 1197....     9d514ce429a7   \n","3    opacity 1 2729 2181.33331 3677.00012 2785.33331     28dddc8559b2   \n","4  opacity 1 623.23328 1050 1337.23328 2156 opaci...     dfd9fdd85a3e   \n","\n","   none_or_box  none  box  fold  \n","0            0     1    0     0  \n","1            1     0    1     2  \n","2            0     1    0     4  \n","3            0     1    0     1  \n","4            0     1    0     0  "]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(6117, 8)"]},"metadata":{},"output_type":"display_data"}],"source":["#COMPETITION_NAME = 'siimcovid19-512-img-png-600-study-png'\n","COMPETITION_NAME = 'siim-image'\n","load_dir = f\"/kaggle/input/{COMPETITION_NAME}/\"\n","df = pd.read_csv(\"/kaggle/input/siim-image/new_resized_data/new_df.csv\")\n","df['none_or_box'] = df['label'].apply(lambda x: 1 if x.split()[0] == 'none' else 0)\n","df[['none','box']] = tf.one_hot(df['none_or_box'].values, 2, dtype = tf.int8)\n","label_cols = df.columns[5:7]\n","print(label_cols)\n","\n","# kfold split\n","gkf = GroupKFold(n_splits = 5)\n","df['fold'] = -1\n","for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df.StudyInstanceUID.tolist())):\n","    df.loc[val_idx, 'fold'] = fold\n","    \n","display(df.head(), df.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.453436Z","iopub.status.busy":"2021-07-31T03:31:49.453082Z","iopub.status.idle":"2021-07-31T03:31:49.460536Z","shell.execute_reply":"2021-07-31T03:31:49.459630Z","shell.execute_reply.started":"2021-07-31T03:31:49.453402Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n","Wall time: 7.63 µs\n"]}],"source":["%%time\n","#training\n","\n","if v1:\n","    trained_model_path = '/kaggle/working/effnetB7_model'\n","    if not os.path.isdir(trained_model_path):\n","        os.makedirs(trained_model_path)\n","    else:\n","        shutil.rmtree(trained_model_path)\n","\n","if v1:\n","    GCS_DS_PATH = KaggleDatasets().get_gcs_path(COMPETITION_NAME) # dataset path for all\n","    # COMPETITION_NAME 으로 지정한 public data folder 를 kaggle datasets 을 활용해서 \n","    # google cloud storage 에 연결하는 코드 -> for using TPU\n","    strategy = auto_select_accelerator()\n","    BATCH_SIZE = strategy.num_replicas_in_sync * 16\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Custom LR scheduler"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.463134Z","iopub.status.busy":"2021-07-31T03:31:49.462737Z","iopub.status.idle":"2021-07-31T03:31:49.474655Z","shell.execute_reply":"2021-07-31T03:31:49.473612Z","shell.execute_reply.started":"2021-07-31T03:31:49.463103Z"},"trusted":true},"outputs":[],"source":["# Configuration\n","# Configuration\n","configurating = False\n","scheduling = False\n","\n","if configurating:\n","    IMAGE_SIZE = [512, 512]\n","    EPOCHS = 20\n","    FOLDS = 3\n","    SEED = 777\n","    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","    AUG_BATCH = BATCH_SIZE\n","    FIRST_FOLD_ONLY = False\n","\n","if scheduling:\n","    # Learning rate schedule for TPU, GPU and CPU.\n","    # Using an LR ramp up because fine-tuning a pre-trained model.\n","    # Starting with a high LR would break the pre-trained weights.\n","    \n","    EPOCHS = 20\n","    \n","    LR_START = 0.00005\n","    LR_MAX = 0.00002 * strategy.num_replicas_in_sync # 8\n","    LR_MIN = 0.00001\n","    LR_RAMPUP_EPOCHS = 5\n","    LR_SUSTAIN_EPOCHS = 0\n","    LR_EXP_DECAY = .8\n","\n","    def lrfn(epoch):\n","        if epoch < LR_RAMPUP_EPOCHS:\n","            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n","        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n","            lr = LR_MAX\n","        else:\n","            lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n","        return lr\n","\n","    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n","\n","    rng = [i for i in range(20 if EPOCHS < 20 else EPOCHS)]\n","    y = [lrfn(x) for x in rng]\n","    plt.plot(rng, y)\n","    print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"]},{"cell_type":"markdown","metadata":{},"source":["# CutMix Augmentation\n","The following code does cutmix using the GPU/TPU. Change the variables `SWITCH`, `CUTMIX_PROB` and `MIXUP_PROB` in function `transform()` to control the amount of augmentation during training. CutMix will occur `SWITCH * CUTMIX_PROB` often and MixUp will occur `(1-SWITCH) * MIXUP_PROB` often during training.\n","\n","[[Notebook] CutMix and MixUp on GPU/TPU](https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.476867Z","iopub.status.busy":"2021-07-31T03:31:49.476484Z","iopub.status.idle":"2021-07-31T03:31:49.500357Z","shell.execute_reply":"2021-07-31T03:31:49.499359Z","shell.execute_reply.started":"2021-07-31T03:31:49.476834Z"},"trusted":true},"outputs":[],"source":["def onehot(image, label):\n","    CLASSES = 2\n","    return image, tf.one_hot(label, CLASSES)\n","\n","# only implement cutmix for testing because\n","def cutmix(image, label, PROBABILITY = 1.0):\n","    # input image - is a batch of images of size [n, dim, dim, 3] not a single image of [dim, dim, 3]\n","    # output - a batch of images with cutmix applied\n","    DIM = 512\n","    CLASSES = 2\n","    AUG_BATCH = 128 if v1 else 64\n","    imgs = []\n","    labs = []\n","\n","    for j in range(AUG_BATCH): # = BATCH_SIZE\n","        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n","        P = tf.cast(tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n","        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n","        k = tf.cast( tf.random.uniform([],0,AUG_BATCH), tf.int32)\n","        \n","        # CHOOSE RANDOM LOCATION\n","        x = tf.cast(tf.random.uniform([],0,DIM), tf.int32)\n","        y = tf.cast(tf.random.uniform([],0,DIM),tf.int32)\n","        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n","        WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\n","        ya = tf.math.maximum(0,y-WIDTH//2)\n","        yb = tf.math.minimum(DIM,y+WIDTH//2)\n","        xa = tf.math.maximum(0,x-WIDTH//2)\n","        xb = tf.math.minimum(DIM,x+WIDTH//2)\n","        \n","        # MAKE CUTMIX IMAGE\n","        one = image[j,ya:yb,0:xa,:]\n","        two = image[k,ya:yb,xa:xb,:]\n","        three = image[j,ya:yb,xb:DIM,:]\n","        middle = tf.concat([one,two,three],axis=1)\n","        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n","        imgs.append(img)\n","        \n","        # MAKE CUTMIX LABEL\n","        a = tf.cast(WIDTH*WIDTH/DIM/DIM, tf.float32)\n","        if len(label.shape)==1:\n","            lab1 = tf.one_hot(label[j],CLASSES)\n","            lab2 = tf.one_hot(label[k],CLASSES)\n","        else:\n","            lab1 = tf.cast(label[j,], tf.float32)\n","            lab2 = tf.cast(label[k,], tf.float32)\n","        labs.append((1-a)*lab1 + a*lab2)\n","        \n","    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n","    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n","    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n","    return image2, label2\n","\n","def transform(image, label):\n","    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n","    DIM = 512\n","    CLASSES = 2\n","    SWITCH = 0.5\n","    CUTMIX_PROB = 0.666\n","    AUG_BATCH = 128 if v1 else 64\n","        \n","    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n","    image2, label2 = cutmix(image, label, CUTMIX_PROB)\n","    imgs = []\n","    labs = []\n","    for j in range(AUG_BATCH):\n","        #P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n","        imgs.append(image2[j,])\n","        labs.append(label2[j,])\n","    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR\n","    image4 = tf.reshape(tf.stack(imgs), (AUG_BATCH, DIM, DIM, 3))\n","    label4 = tf.reshape(tf.stack(labs), (AUG_BATCH, CLASSES))\n","    return image4, label4"]},{"cell_type":"markdown","metadata":{},"source":["# Display CutMix Augmentation"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.502545Z","iopub.status.busy":"2021-07-31T03:31:49.502086Z","iopub.status.idle":"2021-07-31T03:31:49.518610Z","shell.execute_reply":"2021-07-31T03:31:49.517762Z","shell.execute_reply.started":"2021-07-31T03:31:49.502473Z"},"trusted":true},"outputs":[],"source":["display_image = False\n","\n","if display_image:\n","    i = 0\n","    detail_path = '/new_resized_data/image_512/'\n","    valid_paths = GCS_DS_PATH + detail_path + df[df['fold'] == i]['id'] + '.png'\n","    train_paths = GCS_DS_PATH + detail_path + df[df['fold'] != i]['id'] + '.png'\n","    valid_labels = df[df['fold'] == i][label_cols].values\n","    train_labels = df[df['fold'] != i][label_cols].values\n","    img_size = 512\n","    BATCH_SIZE = 128\n","    # train image\n","    decoder = build_decoder(with_labels=True,\n","                            target_size=(img_size, img_size),\n","                            ext='png')\n","    # valid image\n","    test_decoder = build_decoder(with_labels=False, \n","                                 target_size=(img_size, img_size),\n","                                 ext='png')\n","\n","    train_dataset = build_dataset(train_paths,\n","                                  train_labels,\n","                                  bsize=BATCH_SIZE,\n","                                  decode_fn=decoder)\n","\n","    valid_dataset = build_dataset(valid_paths,\n","                                  valid_labels,\n","                                  bsize=BATCH_SIZE,\n","                                  decode_fn=decoder,\n","                                  repeat=False,\n","                                  shuffle=False,\n","                                  augment=False,\n","                                  do_mix=False)\n","    # iterator 가 아니다. repeat() 로 batch 를 읽는다.\n","    sample = train_dataset.repeat()\n","    for (img, label) in sample:\n","        train_img = img[:16]\n","        train_label = label[:16]\n","        break\n","    print('load train dataset')\n","    sample = valid_dataset.repeat()\n","    for (img, label) in sample:\n","        valid_img = img[:16]\n","        valid_label = label[:16]\n","        break\n","    print('load valid dataset')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.520238Z","iopub.status.busy":"2021-07-31T03:31:49.519955Z","iopub.status.idle":"2021-07-31T03:31:49.537898Z","shell.execute_reply":"2021-07-31T03:31:49.536625Z","shell.execute_reply.started":"2021-07-31T03:31:49.520210Z"},"trusted":true},"outputs":[],"source":["if display_image:\n","    fig, axes = plt.subplots(4,4, figsize=(12,12))\n","    for i in range(16):\n","        axes[i//4][i%4].imshow(train_img[i])\n","        axes[i//4][i%4].set_title(f\"{train_label[i].numpy()[1]}\")\n","    plt.show();\n","    \n","    fig, axes = plt.subplots(4,4, figsize=(12,12))\n","    for i in range(16):\n","        axes[i//4][i%4].imshow(valid_img[i])\n","        axes[i//4][i%4].set_title(f\"{valid_label[i].numpy()[1]}\")\n","    plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["# AugMix Augmentation\n","[[Notebook] AugMix Data augmentation on TPU](https://www.kaggle.com/szacho/augmix-data-augmentation-on-tpu)   \n","\n","AugMix 는 구현 실패"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.539682Z","iopub.status.busy":"2021-07-31T03:31:49.539334Z","iopub.status.idle":"2021-07-31T03:31:49.558618Z","shell.execute_reply":"2021-07-31T03:31:49.557662Z","shell.execute_reply.started":"2021-07-31T03:31:49.539643Z"},"trusted":true},"outputs":[],"source":["# Augmix Helper Funtion\n","def int_parameter(level, maxval):\n","    return tf.cast(level * maxval / 10, tf.int32)\n","\n","def float_parameter(level, maxval):\n","    return tf.cast((level) * maxval / 10., tf.float32)\n","\n","def sample_level(n):\n","    return tf.random.uniform(shape=[1], minval=0.1, maxval=n, dtype=tf.float32)\n","    \n","def affine_transform(image, transform_matrix):\n","    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n","    DIM = IMAGE_SIZE[0]\n","    XDIM = DIM%2 #fix for size 331\n","    \n","    x = tf.repeat(tf.range(DIM//2,-DIM//2,-1), DIM)\n","    y = tf.tile(tf.range(-DIM//2,DIM//2), [DIM])\n","    z = tf.ones([DIM*DIM], dtype='int32')\n","    idx = tf.stack([x, y, z])\n","    \n","    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n","    idx2 = K.dot(transform_matrix, tf.cast(idx, dtype='float32'))\n","    idx2 = K.cast(idx2, dtype='int32')\n","    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n","    \n","    # FIND ORIGIN PIXEL VALUES           \n","    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n","    d = tf.gather_nd(image, tf.transpose(idx3))\n","    return tf.reshape(d,[DIM,DIM,3])\n","\n","def blend(image1, image2, factor):\n","    if factor == 0.0:\n","        return tf.convert_to_tensor(image1)\n","    if factor == 1.0:\n","        return tf.convert_to_tensor(image2)\n","\n","    image1 = tf.cast(image1, tf.float32)\n","    image2 = tf.cast(image2, tf.float32)\n","\n","    difference = image2 - image1\n","    scaled = factor * difference\n","\n","    # Do addition in float.\n","    temp = tf.cast(image1, tf.float32) + scaled\n","\n","    # Interpolate\n","    if factor > 0.0 and factor < 1.0:\n","        # Interpolation means we always stay within 0 and 255.\n","        return tf.cast(temp, tf.uint8)\n","\n","    # Extrapolate:\n","    #\n","    # We need to clip and then cast.\n","    return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)"]},{"cell_type":"markdown","metadata":{},"source":["### Transformations\n","These are simple augmentations used by AugMix. Every function takes `image` and `level` (integer from 1 to 10) as arguments. The second one indicates how much variation will particular transformation yield, in other words, how strong it will be.\n","\n","Translate, shear and rotate augmentations are based on [this notebook](https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96)."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.560715Z","iopub.status.busy":"2021-07-31T03:31:49.560249Z","iopub.status.idle":"2021-07-31T03:31:49.614718Z","shell.execute_reply":"2021-07-31T03:31:49.613417Z","shell.execute_reply.started":"2021-07-31T03:31:49.560669Z"},"trusted":true},"outputs":[],"source":["def rotate(image, level):\n","    degrees = float_parameter(sample_level(level), 30)\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    degrees = tf.cond(rand_var > 0.5, lambda: degrees, lambda: -degrees)\n","\n","    angle = math.pi*degrees/180 # convert degrees to radians\n","    angle = tf.cast(angle, tf.float32)\n","    # define rotation matrix\n","    c1 = tf.math.cos(angle)\n","    s1 = tf.math.sin(angle)\n","    one = tf.constant([1],dtype='float32')\n","    zero = tf.constant([0],dtype='float32')\n","    rotation_matrix = tf.reshape(tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one], axis=0), [3,3])\n","\n","    transformed = affine_transform(image, rotation_matrix)\n","    return transformed\n","\n","def translate_x(image, level):\n","    lvl = int_parameter(sample_level(level), IMAGE_SIZE[0] / 3)\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n","\n","    one = tf.constant([1], dtype='float32')\n","    zero = tf.constant([0], dtype='float32')\n","    lvl = tf.cast(lvl, tf.float32)\n","    translate_x_matrix = tf.reshape(tf.concat([one,zero,zero, zero,one,lvl, zero,zero,one], axis=0), [3,3])\n","\n","    transformed = affine_transform(image, translate_x_matrix)\n","    return transformed\n","\n","def translate_y(image, level):\n","    lvl = int_parameter(sample_level(level), IMAGE_SIZE[0] / 3)\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n","\n","    one = tf.constant([1], dtype='float32')\n","    zero = tf.constant([0], dtype='float32')\n","    lvl = tf.cast(lvl, tf.float32)\n","    translate_y_matrix = tf.reshape(tf.concat([one,zero,lvl, zero,one,zero, zero,zero,one], axis=0), [3,3])\n","\n","    transformed = affine_transform(image, translate_y_matrix)\n","    return transformed\n","\n","def shear_x(image, level):\n","    lvl = float_parameter(sample_level(level), 0.3)\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n","\n","    one = tf.constant([1], dtype='float32')\n","    zero = tf.constant([0], dtype='float32')\n","    s2 = tf.math.sin(lvl)\n","    shear_x_matrix = tf.reshape(tf.concat([one,s2,zero, zero,one,zero, zero,zero,one],axis=0), [3,3])   \n","\n","    transformed = affine_transform(image, shear_x_matrix)\n","    return transformed\n","\n","def shear_y(image, level):\n","    lvl = float_parameter(sample_level(level), 0.3)\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n","\n","    one = tf.constant([1], dtype='float32')\n","    zero = tf.constant([0], dtype='float32')\n","    c2 = tf.math.cos(lvl)\n","    shear_y_matrix = tf.reshape(tf.concat([one,zero,zero, zero,c2,zero, zero,zero,one],axis=0), [3,3])   \n","    \n","    transformed = affine_transform(image, shear_y_matrix)\n","    return transformed\n","\n","def solarize(image, level):\n","    # For each pixel in the image, select the pixel\n","    # if the value is less than the threshold.\n","    # Otherwise, subtract 255 from the pixel.\n","    threshold = float_parameter(sample_level(level), 1)\n","    return tf.where(image < threshold, image, 1 - image)\n","\n","def solarize_add(image, level):\n","    # For each pixel in the image less than threshold\n","    # we add 'addition' amount to it and then clip the\n","    # pixel value to be between 0 and 255. The value\n","    # of 'addition' is between -128 and 128.\n","    threshold = float_parameter(sample_level(level), 1)\n","    addition = float_parameter(sample_level(level), 0.5)\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    addition = tf.cond(rand_var > 0.5, lambda: addition, lambda: -addition)\n","\n","    added_image = tf.cast(image, tf.float32) + addition\n","    added_image = tf.cast(tf.clip_by_value(added_image, 0, 1), tf.float32)\n","    return tf.where(image < threshold, added_image, image)\n","\n","def posterize(image, level):\n","    lvl = int_parameter(sample_level(level), 8)\n","    shift = 8 - lvl\n","    shift = tf.cast(shift, tf.uint8)\n","    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n","    image = tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\n","    return tf.cast(tf.clip_by_value(tf.math.divide(image, 255), 0, 1), tf.float32)\n","\n","def autocontrast(image, _):\n","    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n","\n","    def scale_channel(image):\n","        # A possibly cheaper version can be done using cumsum/unique_with_counts\n","        # over the histogram values, rather than iterating over the entire image.\n","        # to compute mins and maxes.\n","        lo = tf.cast(tf.reduce_min(image), tf.float32)\n","        hi = tf.cast(tf.reduce_max(image), tf.float32)\n","\n","        # Scale the image, making the lowest value 0 and the highest value 255.\n","        def scale_values(im):\n","            scale = 255.0 / (hi - lo)\n","            offset = -lo * scale\n","            im = tf.cast(im, tf.float32) * scale + offset\n","            im = tf.clip_by_value(im, 0.0, 255.0)\n","            return tf.cast(im, tf.uint8)\n","\n","        result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n","        return result\n","\n","    # Assumes RGB for now.  Scales each channel independently\n","    # and then stacks the result.\n","    s1 = scale_channel(image[:, :, 0])\n","    s2 = scale_channel(image[:, :, 1])\n","    s3 = scale_channel(image[:, :, 2])\n","    image = tf.stack([s1, s2, s3], 2)\n","    return tf.cast(tf.clip_by_value(tf.math.divide(image, 255), 0, 1), tf.float32)\n","\n","def equalize(image, _):\n","    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n","\n","    def scale_channel(im, c):\n","        im = tf.cast(im[:, :, c], tf.int32)\n","        # Compute the histogram of the image channel.\n","        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","        # For the purposes of computing the step, filter out the nonzeros.\n","        nonzero = tf.where(tf.not_equal(histo, 0))\n","        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","\n","        def build_lut(histo, step):\n","            # Compute the cumulative sum, shifting by step // 2\n","            # and then normalization by step.\n","            lut = (tf.cumsum(histo) + (step // 2)) // step\n","            # Shift lut, prepending with 0.\n","            lut = tf.concat([[0], lut[:-1]], 0)\n","            # Clip the counts to be in range.  This is done\n","            # in the C code for image.point.\n","            return tf.clip_by_value(lut, 0, 255)\n","\n","        # If step is zero, return the original image.  Otherwise, build\n","        # lut from the full histogram and step and then index from it.\n","        result = tf.cond(tf.equal(step, 0),\n","                        lambda: im,\n","                        lambda: tf.gather(build_lut(histo, step), im))\n","\n","        return tf.cast(result, tf.uint8)\n","\n","    # Assumes RGB for now.  Scales each channel independently\n","    # and then stacks the result.\n","    s1 = scale_channel(image, 0)\n","    s2 = scale_channel(image, 1)\n","    s3 = scale_channel(image, 2)\n","    image = tf.stack([s1, s2, s3], 2)\n","\n","    return tf.cast(tf.clip_by_value(tf.math.divide(image, 255), 0, 1), tf.float32)\n","\n","def color(image, level):\n","    factor = float_parameter(sample_level(level), 1.8) + 0.1\n","    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n","    degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n","    blended = blend(degenerate, image, factor)\n","    return tf.cast(tf.clip_by_value(tf.math.divide(blended, 255), 0, 1), tf.float32)\n","\n","def brightness(image, level):\n","    delta = float_parameter(sample_level(level), 0.5) + 0.1\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    delta = tf.cond(rand_var > 0.5, lambda: delta, lambda: -delta) \n","    return tf.image.adjust_brightness(image, delta=delta)\n","\n","def contrast(image, level):\n","    factor = float_parameter(sample_level(level), 1.8) + 0.1\n","    factor = tf.reshape(factor, [])\n","    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n","    factor = tf.cond(rand_var > 0.5, lambda: factor, lambda: 1.9 - factor  )\n","\n","    return tf.image.adjust_contrast(image, factor)\n"]},{"cell_type":"markdown","metadata":{},"source":["### AugMix"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.616626Z","iopub.status.busy":"2021-07-31T03:31:49.616287Z","iopub.status.idle":"2021-07-31T03:31:49.646679Z","shell.execute_reply":"2021-07-31T03:31:49.645435Z","shell.execute_reply.started":"2021-07-31T03:31:49.616594Z"},"trusted":true},"outputs":[],"source":["means = {'R': 0.44892993872313053, 'G': 0.4148519066242368, 'B': 0.301880284715257}\n","stds = {'R': 0.24393544875614917, 'G': 0.2108791383467354, 'B': 0.220427056859487}\n","\n","def substract_means(image):\n","    image = image - np.array([means['R'], means['G'], means['B']])\n","    return image\n","\n","def normalize(image):\n","    image = substract_means(image)\n","    image = image / np.array([stds['R'], stds['G'], stds['B']])\n","    return tf.clip_by_value(image, 0, 1)\n","\n","def apply_op(image, level, which):\n","    # is there any better way than manually typing all of these conditions? \n","    # I tried to randomly select transformation from array of functions, but tensorflow didn't let me to\n","    augmented = image\n","    augmented = tf.cond(which == tf.constant([0], dtype=tf.int32), lambda: rotate(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([1], dtype=tf.int32), lambda: translate_x(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([2], dtype=tf.int32), lambda: translate_y(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([3], dtype=tf.int32), lambda: shear_x(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([4], dtype=tf.int32), lambda: shear_y(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([5], dtype=tf.int32), lambda: solarize_add(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([6], dtype=tf.int32), lambda: solarize(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([7], dtype=tf.int32), lambda: posterize(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([8], dtype=tf.int32), lambda: autocontrast(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([9], dtype=tf.int32), lambda: equalize(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([10], dtype=tf.int32), lambda: color(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([11], dtype=tf.int32), lambda: contrast(image, level), lambda: augmented)\n","    augmented = tf.cond(which == tf.constant([12], dtype=tf.int32), lambda: brightness(image, level), lambda: augmented)\n","    return augmented\n","\n","def augmix(image):\n","    # you can play with these parameters\n","    severity = 3 # level of transformations as described above in transformations (integer from 1 to 10)\n","    width = 3 # number of different chains of transformations to be mixed\n","    depth = -1 # number of transformations in one chain, -1 means random from 1 to 3\n","    \n","    alpha = 1.\n","    dir_dist = tfp.distributions.Dirichlet([alpha]*width)\n","    ws = tf.cast(dir_dist.sample(), tf.float32)\n","    beta_dist = tfp.distributions.Beta(alpha, alpha)\n","    m = tf.cast(beta_dist.sample(), tf.float32)\n","\n","    mix = tf.zeros_like(image, dtype='float32')\n","\n","    def outer_loop_cond(i, depth, mix):\n","        return tf.less(i, width)\n","\n","    def outer_loop_body(i, depth, mix):\n","        image_aug = tf.identity(image)\n","        depth = tf.cond(tf.greater(depth, 0), lambda: depth, lambda: tf.random.uniform(shape=[], minval=1, maxval=3, dtype=tf.int32))\n","\n","        def inner_loop_cond(j, image_aug):\n","            return tf.less(j, depth)\n","\n","        def inner_loop_body(j, image_aug):\n","            which = tf.random.uniform(shape=[], minval=0, maxval=3, dtype=tf.int32)\n","            image_aug = apply_op(image_aug, severity, which)\n","            j = tf.add(j, 1)\n","            return j, image_aug\n","        \n","        j = tf.constant([0], dtype=tf.int32)\n","        j, image_aug = tf.while_loop(inner_loop_cond, inner_loop_body, [j, image_aug])\n","\n","        wsi = tf.gather(ws, i)\n","        mix = tf.add(mix, wsi*normalize(image_aug))\n","        i = tf.add(i, 1)\n","        return i, depth, mix\n","\n","    i = tf.constant([0], dtype=tf.int32)\n","    i, depth, mix = tf.while_loop(outer_loop_cond, outer_loop_body, [i, depth, mix])\n","    \n","    mixed = tf.math.scalar_mul((1 - m), normalize(image)) + tf.math.scalar_mul(m, mix)\n","    return tf.clip_by_value(mixed, 0, 1)"]},{"cell_type":"markdown","metadata":{},"source":["# Build, Train, Infer Model\n","This is the 5-Fold workflow copied from Ragnar's notebook here. Now we add cutmix and/or mixup augmentation to the training images on the fly! Since cutmix and/or mixup returns one hot encoded labels, we change the loss and metric to `categorical_crossentropy` instead of `sparse_categorical_crossentropy`.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# EfficientNetB7"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.648583Z","iopub.status.busy":"2021-07-31T03:31:49.648107Z","iopub.status.idle":"2021-07-31T03:31:49.670460Z","shell.execute_reply":"2021-07-31T03:31:49.669149Z","shell.execute_reply.started":"2021-07-31T03:31:49.648536Z"},"trusted":true},"outputs":[],"source":["ONE_FOLD = False\n","if v1:\n","    for i in range(5):\n","        print(f\"\\n ---- Fold {i} start ---- \\n\")\n","        detail_path = '/new_resized_data/image_512/'\n","        valid_paths = GCS_DS_PATH + detail_path + df[df['fold'] == i]['id'] + '.png'\n","        train_paths = GCS_DS_PATH + detail_path + df[df['fold'] != i]['id'] + '.png'\n","        valid_labels = df[df['fold'] == i][label_cols].values\n","        train_labels = df[df['fold'] != i][label_cols].values\n","\n","        IMSIZE = (224, 240, 260, 300, 380, 456, 528, 600, 512)\n","        IMS = 8 # image_level file 을 사용하므로 img_size = 512\n","        \n","        \n","        decoder = build_decoder(with_labels=True,\n","                                target_size=(IMSIZE[IMS], IMSIZE[IMS]),\n","                                ext='png')\n","        \n","        test_decoder = build_decoder(with_labels=False, \n","                                     target_size=(IMSIZE[IMS], IMSIZE[IMS]),\n","                                     ext='png')\n","\n","        train_dataset = build_dataset(train_paths,\n","                                      train_labels,\n","                                      bsize=BATCH_SIZE,\n","                                      decode_fn=decoder)\n","        \n","        valid_dataset = build_dataset(valid_paths,\n","                                      valid_labels,\n","                                      bsize=BATCH_SIZE,\n","                                      decode_fn=decoder,\n","                                      repeat=False,\n","                                      shuffle=False,\n","                                      augment=False,\n","                                      do_mix=False)\n","        try:\n","            n_labels = train_labels.shape[1]\n","        except:\n","            n_labels = 1\n","        print(f\"\\n n_labels : {n_labels} \\n\")\n","        with strategy.scope():\n","            model = tf.keras.Sequential([\n","                efn.EfficientNetB7(input_shape = (IMSIZE[IMS], IMSIZE[IMS], 3),\n","                                   weights = 'imagenet',\n","                                   include_top=False),\n","                tf.keras.layers.GlobalAveragePooling2D(),\n","                tf.keras.layers.Dense(n_labels, activation='softmax')\n","            ])\n","            model.compile(optimizer = tf.keras.optimizers.Adam(),\n","                          loss='categorical_crossentropy', # 'binary_crossentropy'\n","                          metrics = [tf.keras.metrics.AUC(multi_label=True)])\n","            model.summary()\n","\n","        steps_per_epoch = train_paths.shape[0] // BATCH_SIZE \n","        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{trained_model_path}/model{i}_2class.h5', \n","                                                        save_best_only=True, \n","                                                        monitor='val_loss',\n","                                                        mode='min')\n","        \n","        lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n","                                                          patience=3,\n","                                                          min_lr=1e-6,\n","                                                          factor=0.2,\n","                                                          mode='min')\n","        \n","        annealing = tf.keras.experimental.CosineDecayRestarts(initial_learning_rate=0.0001,\n","                                                              first_decay_steps=10,\n","                                                              t_mul=1.0,\n","                                                              m_mul=1.0,\n","                                                              alpha=0.0)\n","        lr_callback = tf.keras.callbacks.LearningRateScheduler(annealing, verbose = True)\n","        \n","        history = model.fit(train_dataset,\n","                            epochs=20,\n","                            verbose=1,\n","                            callbacks=[checkpoint, lr_reducer, annealing],\n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=valid_dataset)\n","        hist_df = pd.DataFrame(history.history)\n","        hist_df.to_csv(f\"{trained_model_path}/history{i}_2class.csv\")\n","        if ONE_FOLD:\n","            break"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.672301Z","iopub.status.busy":"2021-07-31T03:31:49.671924Z","iopub.status.idle":"2021-07-31T03:31:49.686862Z","shell.execute_reply":"2021-07-31T03:31:49.685575Z","shell.execute_reply.started":"2021-07-31T03:31:49.672266Z"},"trusted":true},"outputs":[],"source":["# comment\n","# target val_AUC : 0.892, val_loss : 0.394 (ratio 2 with LB)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.688732Z","iopub.status.busy":"2021-07-31T03:31:49.688413Z","iopub.status.idle":"2021-07-31T03:31:49.699824Z","shell.execute_reply":"2021-07-31T03:31:49.698733Z","shell.execute_reply.started":"2021-07-31T03:31:49.688701Z"},"trusted":true},"outputs":[],"source":["if False:\n","    history_lst = sorted(glob('./effnetB7_model/*.csv'))\n","    loss_lst = []\n","    auc_lst = []\n","    for i, his in enumerate(history_lst):\n","        tmp = pd.read_csv(his)\n","        tmp = tmp[tmp['val_loss']==tmp['val_loss'].min()]\n","        loss_lst.append(tmp['val_loss'].values)\n","        t = 'val_auc' if i == 0 else f\"val_auc_{i}\"\n","        auc_lst.append(tmp[t])\n","    print(loss_lst, np.mean(loss_lst))\n","# cutmix : 0.394 -> 0.387, auc 는 더 하락 -> LB 엄청 하락? why? AUC 기준으로 생각하자."]},{"cell_type":"markdown","metadata":{},"source":["# Validation"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.702699Z","iopub.status.busy":"2021-07-31T03:31:49.702339Z","iopub.status.idle":"2021-07-31T03:31:49.711135Z","shell.execute_reply":"2021-07-31T03:31:49.710248Z","shell.execute_reply.started":"2021-07-31T03:31:49.702667Z"},"trusted":true},"outputs":[],"source":["predict_valid = False\n","if predict_valid:\n","    model_lst = []\n","    for i in range(5):\n","        model = tf.keras.models.load_model(\n","            f'../input/effnetb7-cutmix/kaggle/working/effnetB7_model/model{i}_2class.h5'\n","            )\n","        model_lst.append(model)\n","    print(model_lst)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.713044Z","iopub.status.busy":"2021-07-31T03:31:49.712618Z","iopub.status.idle":"2021-07-31T03:31:49.722393Z","shell.execute_reply":"2021-07-31T03:31:49.721609Z","shell.execute_reply.started":"2021-07-31T03:31:49.712984Z"},"trusted":true},"outputs":[],"source":["if predict_valid:\n","    # valid image\n","    img_size = 512\n","    decoder = build_decoder(with_labels=False, \n","                            target_size=(img_size, img_size),\n","                            ext='png')\n","    all_dataset = []\n","    all_labels = []\n","    for i in range(5):\n","        print(f\"\\n ---- Fold {i} start ---- \\n\")\n","        detail_path = '/new_resized_data/image_512/'\n","        valid_paths = GCS_DS_PATH + detail_path + df[df['fold'] == i]['id'] + '.png'\n","        valid_labels = df[df['fold'] == i][label_cols].values\n","        \n","        valid_dataset = build_dataset(valid_paths, # without \n","                                      bsize=BATCH_SIZE,\n","                                      decode_fn=decoder,\n","                                      repeat=False,\n","                                      shuffle=False,\n","                                      augment=False,\n","                                      do_mix=False)\n","        all_dataset.append(valid_dataset)\n","        all_labels.append(valid_labels)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.723974Z","iopub.status.busy":"2021-07-31T03:31:49.723562Z","iopub.status.idle":"2021-07-31T03:31:49.739604Z","shell.execute_reply":"2021-07-31T03:31:49.738812Z","shell.execute_reply.started":"2021-07-31T03:31:49.723934Z"},"trusted":true},"outputs":[],"source":["if predict_valid:\n","    scores = []\n","    for i, model in enumerate(model_lst):\n","        print(f\"model {i} start\")\n","        for valid_data, valid_label in zip(all_dataset, all_labels):\n","            result = model.predict(valid_data)[:,1]\n","            sample_score = roc_auc_score(valid_label[:,1], result)\n","            scores.append(sample_score)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.741137Z","iopub.status.busy":"2021-07-31T03:31:49.740728Z","iopub.status.idle":"2021-07-31T03:31:49.751941Z","shell.execute_reply":"2021-07-31T03:31:49.750994Z","shell.execute_reply.started":"2021-07-31T03:31:49.741105Z"},"trusted":true},"outputs":[],"source":["if predict_valid:\n","    result_df = pd.DataFrame({'pred':result[:,1],\n","                              'label':valid_label[:,1]})\n","    result_df.to_csv(\"result_df.csv\", index=False)\n","    zero = result_df[result_df['label']==0]\n","    one = result_df[result_df['label']==1]\n","    plt.hist(zero['pred'], bins=20, alpha=0.3, label='pred for answer zero')\n","    plt.hist(one['pred'], bins=20, alpha=0.3, label='pred for answer one')\n","    plt.show();\n","    \n","    plt.figure(figsize=(14,6))\n","    for col in hist_df.columns:\n","        value = hist_df[col].values.copy()\n","        if col == 'lr':\n","            value *= 100\n","            col = f\"{col} * 100\"\n","        plt.plot(value, alpha=0.6, marker='o', markersize=3, label=col)\n","    plt.legend()\n","    plt.grid()\n","    plt.show();\n","    \n","    del model"]},{"cell_type":"markdown","metadata":{},"source":["# EfficientNetV2_XL"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:49.757240Z","iopub.status.busy":"2021-07-31T03:31:49.756536Z","iopub.status.idle":"2021-07-31T03:31:50.084654Z","shell.execute_reply":"2021-07-31T03:31:50.083539Z","shell.execute_reply.started":"2021-07-31T03:31:49.757187Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["gs://kds-40f526733933c01f1885087fe24fcb06ce3f5c601cd74b3d084cfe68/tfhub_models/efficientnetv2-l-21k-ft1k/feature_vector\n"]}],"source":["# We first find the GCS path of the selected EffNetV2 architecture from the EffNetV2 weights Kaggle dataset\n","\n","if v2:\n","    # Get the Tensorflow Hub model URL\n","    hub_type = 'feature_vector' # ['classification', 'feature_vector']\n","    model_arch = \"efficientnetv2-l-21k-ft1k\"\n","    # Get the GCS path of EfficientNet Models\n","    DS_GCS_PATH = KaggleDatasets().get_gcs_path('efficientnetv2-tfhub-weight-files')\n","    MODEL_GCS_PATH = f\"{DS_GCS_PATH}/tfhub_models/{model_arch}/{hub_type}\"\n","    print(MODEL_GCS_PATH)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:50.089462Z","iopub.status.busy":"2021-07-31T03:31:50.088691Z","iopub.status.idle":"2021-07-31T03:31:50.097412Z","shell.execute_reply":"2021-07-31T03:31:50.096058Z","shell.execute_reply.started":"2021-07-31T03:31:50.089410Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model path\n"]}],"source":["if v2:\n","    trained_model_path = '/kaggle/working/effnetV2_model'\n","    if not os.path.isdir(trained_model_path):\n","        os.makedirs(trained_model_path)\n","    else:\n","        shutil.rmtree(trained_model_path)\n","    print('model path')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T03:31:55.526426Z","iopub.status.busy":"2021-07-31T03:31:55.525904Z","iopub.status.idle":"2021-07-31T05:06:37.908861Z","shell.execute_reply":"2021-07-31T05:06:37.907530Z","shell.execute_reply.started":"2021-07-31T03:31:55.526391Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Name : siim-image\n","Running on TPU: grpc://10.0.0.2:8470\n","Running on 8 replicas\n","Fold 0 start\n","\n"," n_labels : 2 \n","\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","keras_layer (KerasLayer)     (None, 1280)              117746848 \n","_________________________________________________________________\n","dropout (Dropout)            (None, 1280)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 2562      \n","=================================================================\n","Total params: 117,749,410\n","Trainable params: 117,236,834\n","Non-trainable params: 512,576\n","_________________________________________________________________\n","Epoch 1/20\n","76/76 [==============================] - 351s 2s/step - loss: 0.6154 - auc: 0.7431 - val_loss: 0.7437 - val_auc: 0.8416\n","Epoch 2/20\n","76/76 [==============================] - 34s 444ms/step - loss: 0.5400 - auc: 0.8125 - val_loss: 0.4348 - val_auc: 0.8818\n","Epoch 3/20\n","76/76 [==============================] - 34s 443ms/step - loss: 0.5252 - auc: 0.8245 - val_loss: 0.4198 - val_auc: 0.8997\n","Epoch 4/20\n","76/76 [==============================] - 34s 443ms/step - loss: 0.5054 - auc: 0.8386 - val_loss: 0.4259 - val_auc: 0.9056\n","Epoch 5/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.5008 - auc: 0.8461 - val_loss: 0.4300 - val_auc: 0.8856\n","Epoch 6/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4997 - auc: 0.8405 - val_loss: 0.4214 - val_auc: 0.8890\n","Epoch 7/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.4690 - auc: 0.8674 - val_loss: 0.3621 - val_auc: 0.9187\n","Epoch 8/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4550 - auc: 0.8753 - val_loss: 0.3464 - val_auc: 0.9261\n","Epoch 9/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4371 - auc: 0.8833 - val_loss: 0.3468 - val_auc: 0.9255\n","Epoch 10/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4449 - auc: 0.8770 - val_loss: 0.3627 - val_auc: 0.9187\n","Epoch 11/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4330 - auc: 0.8904 - val_loss: 0.3531 - val_auc: 0.9236\n","Epoch 12/20\n","76/76 [==============================] - 33s 438ms/step - loss: 0.4113 - auc: 0.8977 - val_loss: 0.3600 - val_auc: 0.9211\n","Epoch 13/20\n","76/76 [==============================] - 33s 438ms/step - loss: 0.3977 - auc: 0.9060 - val_loss: 0.3606 - val_auc: 0.9206\n","Epoch 14/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4010 - auc: 0.9049 - val_loss: 0.3592 - val_auc: 0.9219\n","Epoch 15/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.3846 - auc: 0.9127 - val_loss: 0.3597 - val_auc: 0.9216\n","Epoch 16/20\n","76/76 [==============================] - 33s 438ms/step - loss: 0.4037 - auc: 0.9011 - val_loss: 0.3590 - val_auc: 0.9218\n","Epoch 17/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3821 - auc: 0.9079 - val_loss: 0.3596 - val_auc: 0.9213\n","Epoch 18/20\n","76/76 [==============================] - 33s 438ms/step - loss: 0.4154 - auc: 0.9040 - val_loss: 0.3592 - val_auc: 0.9213\n","Epoch 19/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4013 - auc: 0.9013 - val_loss: 0.3605 - val_auc: 0.9215\n","Epoch 20/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3903 - auc: 0.9093 - val_loss: 0.3619 - val_auc: 0.9209\n","Fold 1 start\n","\n"," n_labels : 2 \n","\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","keras_layer_1 (KerasLayer)   (None, 1280)              117746848 \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 1280)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 2562      \n","=================================================================\n","Total params: 117,749,410\n","Trainable params: 117,236,834\n","Non-trainable params: 512,576\n","_________________________________________________________________\n","Epoch 1/20\n","76/76 [==============================] - 320s 1s/step - loss: 0.6233 - auc_1: 0.7539 - val_loss: 0.5034 - val_auc_1: 0.8582\n","Epoch 2/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.5116 - auc_1: 0.8354 - val_loss: 0.4671 - val_auc_1: 0.8582\n","Epoch 3/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.5196 - auc_1: 0.8308 - val_loss: 0.4756 - val_auc_1: 0.8762\n","Epoch 4/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4937 - auc_1: 0.8502 - val_loss: 0.5703 - val_auc_1: 0.8425\n","Epoch 5/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4995 - auc_1: 0.8426 - val_loss: 0.4111 - val_auc_1: 0.8979\n","Epoch 6/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4779 - auc_1: 0.8548 - val_loss: 0.4051 - val_auc_1: 0.8970\n","Epoch 7/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.5010 - auc_1: 0.8485 - val_loss: 0.3831 - val_auc_1: 0.9095\n","Epoch 8/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.4908 - auc_1: 0.8528 - val_loss: 0.4241 - val_auc_1: 0.8965\n","Epoch 9/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4709 - auc_1: 0.8596 - val_loss: 0.3883 - val_auc_1: 0.9049\n","Epoch 10/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.4717 - auc_1: 0.8647 - val_loss: 0.4143 - val_auc_1: 0.8914\n","Epoch 11/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4597 - auc_1: 0.8708 - val_loss: 0.3862 - val_auc_1: 0.9061\n","Epoch 12/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4239 - auc_1: 0.8924 - val_loss: 0.3818 - val_auc_1: 0.9088\n","Epoch 13/20\n","76/76 [==============================] - 34s 444ms/step - loss: 0.4176 - auc_1: 0.8928 - val_loss: 0.3906 - val_auc_1: 0.9059\n","Epoch 14/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4048 - auc_1: 0.9058 - val_loss: 0.4125 - val_auc_1: 0.8943\n","Epoch 15/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.3905 - auc_1: 0.9145 - val_loss: 0.3939 - val_auc_1: 0.9043\n","Epoch 16/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.3886 - auc_1: 0.9138 - val_loss: 0.3956 - val_auc_1: 0.9050\n","Epoch 17/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3806 - auc_1: 0.9111 - val_loss: 0.3972 - val_auc_1: 0.9042\n","Epoch 18/20\n","76/76 [==============================] - 34s 441ms/step - loss: 0.3613 - auc_1: 0.9212 - val_loss: 0.3976 - val_auc_1: 0.9042\n","Epoch 19/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3862 - auc_1: 0.9159 - val_loss: 0.3967 - val_auc_1: 0.9041\n","Epoch 20/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3634 - auc_1: 0.9232 - val_loss: 0.3975 - val_auc_1: 0.9048\n","Fold 2 start\n","\n"," n_labels : 2 \n","\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","keras_layer_2 (KerasLayer)   (None, 1280)              117746848 \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 1280)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 2562      \n","=================================================================\n","Total params: 117,749,410\n","Trainable params: 117,236,834\n","Non-trainable params: 512,576\n","_________________________________________________________________\n","Epoch 1/20\n","76/76 [==============================] - 316s 1s/step - loss: 0.6324 - auc_2: 0.7557 - val_loss: 0.4982 - val_auc_2: 0.8460\n","Epoch 2/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.5054 - auc_2: 0.8357 - val_loss: 0.4775 - val_auc_2: 0.8611\n","Epoch 3/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.5134 - auc_2: 0.8327 - val_loss: 0.4690 - val_auc_2: 0.8590\n","Epoch 4/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4864 - auc_2: 0.8541 - val_loss: 0.4561 - val_auc_2: 0.8808\n","Epoch 5/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4942 - auc_2: 0.8518 - val_loss: 0.4542 - val_auc_2: 0.8677\n","Epoch 6/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.5017 - auc_2: 0.8500 - val_loss: 0.4267 - val_auc_2: 0.8852\n","Epoch 7/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.4727 - auc_2: 0.8627 - val_loss: 0.4325 - val_auc_2: 0.8786\n","Epoch 8/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4795 - auc_2: 0.8673 - val_loss: 0.4882 - val_auc_2: 0.8417\n","Epoch 9/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4859 - auc_2: 0.8510 - val_loss: 0.4531 - val_auc_2: 0.8769\n","Epoch 10/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.4445 - auc_2: 0.8838 - val_loss: 0.4252 - val_auc_2: 0.8875\n","Epoch 11/20\n","76/76 [==============================] - 34s 441ms/step - loss: 0.4212 - auc_2: 0.8914 - val_loss: 0.4237 - val_auc_2: 0.8901\n","Epoch 12/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.4184 - auc_2: 0.8932 - val_loss: 0.4313 - val_auc_2: 0.8889\n","Epoch 13/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3863 - auc_2: 0.9102 - val_loss: 0.4312 - val_auc_2: 0.8849\n","Epoch 14/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.3662 - auc_2: 0.9179 - val_loss: 0.4424 - val_auc_2: 0.8835\n","Epoch 15/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3684 - auc_2: 0.9214 - val_loss: 0.4388 - val_auc_2: 0.8844\n","Epoch 16/20\n","76/76 [==============================] - 33s 438ms/step - loss: 0.3716 - auc_2: 0.9205 - val_loss: 0.4415 - val_auc_2: 0.8847\n","Epoch 17/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.3499 - auc_2: 0.9252 - val_loss: 0.4457 - val_auc_2: 0.8840\n","Epoch 18/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.3617 - auc_2: 0.9219 - val_loss: 0.4429 - val_auc_2: 0.8849\n","Epoch 19/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3819 - auc_2: 0.9173 - val_loss: 0.4450 - val_auc_2: 0.8842\n","Epoch 20/20\n","76/76 [==============================] - 33s 438ms/step - loss: 0.3476 - auc_2: 0.9271 - val_loss: 0.4464 - val_auc_2: 0.8847\n","Fold 3 start\n","\n"," n_labels : 2 \n","\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","keras_layer_3 (KerasLayer)   (None, 1280)              117746848 \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 1280)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 2)                 2562      \n","=================================================================\n","Total params: 117,749,410\n","Trainable params: 117,236,834\n","Non-trainable params: 512,576\n","_________________________________________________________________\n","Epoch 1/20\n","76/76 [==============================] - 317s 1s/step - loss: 0.6356 - auc_3: 0.7497 - val_loss: 0.4989 - val_auc_3: 0.8710\n","Epoch 2/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.5440 - auc_3: 0.8125 - val_loss: 0.4805 - val_auc_3: 0.8547\n","Epoch 3/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.5170 - auc_3: 0.8311 - val_loss: 0.4430 - val_auc_3: 0.8748\n","Epoch 4/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.5145 - auc_3: 0.8320 - val_loss: 0.4341 - val_auc_3: 0.8907\n","Epoch 5/20\n","76/76 [==============================] - 34s 443ms/step - loss: 0.5024 - auc_3: 0.8434 - val_loss: 0.4183 - val_auc_3: 0.8905\n","Epoch 6/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4826 - auc_3: 0.8613 - val_loss: 0.3897 - val_auc_3: 0.9067\n","Epoch 7/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4870 - auc_3: 0.8512 - val_loss: 0.4193 - val_auc_3: 0.8984\n","Epoch 8/20\n","76/76 [==============================] - 34s 447ms/step - loss: 0.4830 - auc_3: 0.8568 - val_loss: 0.4138 - val_auc_3: 0.8934\n","Epoch 9/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4794 - auc_3: 0.8584 - val_loss: 0.4758 - val_auc_3: 0.8527\n","Epoch 10/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4435 - auc_3: 0.8741 - val_loss: 0.3933 - val_auc_3: 0.9042\n","Epoch 11/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.4277 - auc_3: 0.8890 - val_loss: 0.3925 - val_auc_3: 0.9042\n","Epoch 12/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4108 - auc_3: 0.9004 - val_loss: 0.3978 - val_auc_3: 0.9013\n","Epoch 13/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4000 - auc_3: 0.9003 - val_loss: 0.3981 - val_auc_3: 0.9030\n","Epoch 14/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.4023 - auc_3: 0.9002 - val_loss: 0.3941 - val_auc_3: 0.9040\n","Epoch 15/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.4010 - auc_3: 0.9031 - val_loss: 0.3941 - val_auc_3: 0.9043\n","Epoch 16/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3961 - auc_3: 0.9084 - val_loss: 0.3960 - val_auc_3: 0.9036\n","Epoch 17/20\n","76/76 [==============================] - 33s 438ms/step - loss: 0.4078 - auc_3: 0.9013 - val_loss: 0.3939 - val_auc_3: 0.9045\n","Epoch 18/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.3883 - auc_3: 0.9078 - val_loss: 0.3974 - val_auc_3: 0.9026\n","Epoch 19/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.4035 - auc_3: 0.9047 - val_loss: 0.3964 - val_auc_3: 0.9041\n","Epoch 20/20\n","76/76 [==============================] - 33s 439ms/step - loss: 0.3917 - auc_3: 0.9051 - val_loss: 0.3963 - val_auc_3: 0.9033\n","Fold 4 start\n","\n"," n_labels : 2 \n","\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","keras_layer_4 (KerasLayer)   (None, 1280)              117746848 \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 1280)              0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 2)                 2562      \n","=================================================================\n","Total params: 117,749,410\n","Trainable params: 117,236,834\n","Non-trainable params: 512,576\n","_________________________________________________________________\n","Epoch 1/20\n","76/76 [==============================] - 327s 1s/step - loss: 0.6263 - auc_4: 0.7515 - val_loss: 0.5034 - val_auc_4: 0.8788\n","Epoch 2/20\n","76/76 [==============================] - 34s 443ms/step - loss: 0.5325 - auc_4: 0.8203 - val_loss: 0.4609 - val_auc_4: 0.8751\n","Epoch 3/20\n","76/76 [==============================] - 34s 443ms/step - loss: 0.4991 - auc_4: 0.8383 - val_loss: 0.4159 - val_auc_4: 0.8902\n","Epoch 4/20\n","76/76 [==============================] - 34s 444ms/step - loss: 0.5085 - auc_4: 0.8354 - val_loss: 0.4388 - val_auc_4: 0.8799\n","Epoch 5/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.5137 - auc_4: 0.8365 - val_loss: 0.4581 - val_auc_4: 0.8763\n","Epoch 6/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.4971 - auc_4: 0.8441 - val_loss: 0.4490 - val_auc_4: 0.8803\n","Epoch 7/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.4957 - auc_4: 0.8459 - val_loss: 0.3904 - val_auc_4: 0.9042\n","Epoch 8/20\n","76/76 [==============================] - 34s 445ms/step - loss: 0.4549 - auc_4: 0.8738 - val_loss: 0.3899 - val_auc_4: 0.9044\n","Epoch 9/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.4479 - auc_4: 0.8792 - val_loss: 0.3816 - val_auc_4: 0.9086\n","Epoch 10/20\n","76/76 [==============================] - 34s 447ms/step - loss: 0.4512 - auc_4: 0.8792 - val_loss: 0.3914 - val_auc_4: 0.9051\n","Epoch 11/20\n","76/76 [==============================] - 34s 446ms/step - loss: 0.4145 - auc_4: 0.8913 - val_loss: 0.3960 - val_auc_4: 0.9003\n","Epoch 12/20\n","76/76 [==============================] - 34s 444ms/step - loss: 0.4345 - auc_4: 0.8829 - val_loss: 0.3949 - val_auc_4: 0.9029\n","Epoch 13/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.4113 - auc_4: 0.8968 - val_loss: 0.3970 - val_auc_4: 0.9026\n","Epoch 14/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.4083 - auc_4: 0.8960 - val_loss: 0.3937 - val_auc_4: 0.9044\n","Epoch 15/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.3865 - auc_4: 0.9109 - val_loss: 0.3937 - val_auc_4: 0.9045\n","Epoch 16/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.3953 - auc_4: 0.9101 - val_loss: 0.3920 - val_auc_4: 0.9054\n","Epoch 17/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.3815 - auc_4: 0.9135 - val_loss: 0.3937 - val_auc_4: 0.9053\n","Epoch 18/20\n","76/76 [==============================] - 34s 442ms/step - loss: 0.4061 - auc_4: 0.9041 - val_loss: 0.3932 - val_auc_4: 0.9053\n","Epoch 19/20\n","76/76 [==============================] - 33s 441ms/step - loss: 0.3959 - auc_4: 0.9075 - val_loss: 0.3927 - val_auc_4: 0.9048\n","Epoch 20/20\n","76/76 [==============================] - 33s 440ms/step - loss: 0.3922 - auc_4: 0.9102 - val_loss: 0.3943 - val_auc_4: 0.9044\n"]}],"source":["ONE_FOLD = False\n","if v2:\n","    print(f\"Dataset Name : {COMPETITION_NAME}\")\n","    strategy = auto_select_accelerator() #Define TPU strategy and clear TPU - try to select TPU else GPU or CPU\n","    GCS_DS_PATH = KaggleDatasets().get_gcs_path(COMPETITION_NAME)\n","    BATCH_SIZE = strategy.num_replicas_in_sync * 8\n","    for i in range(5):\n","        print(f\"Fold {i} start\")        \n","        # Converting global config class object to a dictionary to log using Wandb\n","        # --- remove ---\n","        detail_path = '/new_resized_data/image_512/'\n","        valid_paths = GCS_DS_PATH + detail_path + df[df['fold'] == i]['id'] + '.png'\n","        train_paths = GCS_DS_PATH + detail_path + df[df['fold'] != i]['id'] + '.png'\n","        valid_labels = df[df['fold'] == i][label_cols].values\n","        train_labels = df[df['fold'] != i][label_cols].values\n","        \n","        IMSIZE = (224, 240, 260, 300, 380, 456, 528, 600, 512)\n","        IMS = 8 \n","        # image_level file 을 사용하므로 img_size = 512\n","        \n","        # train image decoder\n","        decoder = build_decoder(with_labels=True,\n","                                target_size=(IMSIZE[IMS], IMSIZE[IMS]),\n","                                ext='png')\n","        # valid build decoder\n","        test_decoder = build_decoder(with_labels=False,\n","                                     target_size=(IMSIZE[IMS], IMSIZE[IMS]),\n","                                     ext='png')\n","           \n","        train_dataset = build_dataset(train_paths,\n","                                      train_labels,\n","                                      bsize=BATCH_SIZE,\n","                                      decode_fn=decoder)\n","        \n","        valid_dataset = build_dataset(valid_paths,\n","                                      valid_labels,\n","                                      bsize=BATCH_SIZE,\n","                                      decode_fn=decoder,\n","                                      repeat=False,\n","                                      shuffle=False,\n","                                      augment=False,\n","                                      do_mix=False)\n","        \n","        try:\n","            n_labels = train_labels.shape[1]\n","        except:\n","            n_labels = 1\n","        print(f\"\\n n_labels : {n_labels} \\n\")\n","        \n","        # n_labels 부분은 num_classes -> for Dense output size\n","        \n","        with strategy.scope():\n","            model = tf.keras.Sequential([\n","                # Explicitly define the input shape so the model can be properly\n","                # loaded by the TFLite Converter (input layer 를 명시적으로 설정)\n","                tf.keras.layers.InputLayer(input_shape = [IMSIZE[IMS],IMSIZE[IMS],3]),\n","                tfhub.KerasLayer(MODEL_GCS_PATH, trainable=True),\n","                tf.keras.layers.Dropout(rate=0.1),\n","                tf.keras.layers.Dense(n_labels, activation='sigmoid')\n","            ])\n","            model.compile(optimizer = tf.keras.optimizers.Adam(),\n","                          loss='binary_crossentropy',\n","                          metrics = [tf.keras.metrics.AUC(multi_label=False)])\n","            model.summary()\n","            \n","        steps_per_epoch = train_paths.shape[0] // BATCH_SIZE \n","        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{trained_model_path}/model{i}_2class.h5', \n","                                                        save_best_only=True, \n","                                                        monitor='val_loss',\n","                                                        mode='min')\n","\n","        lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n","                                                          patience=3,\n","                                                          min_lr=1e-6,\n","                                                          mode='min')\n","        annealing = tf.keras.experimental.CosineDecayRestarts(initial_learning_rate=0.0001,\n","                                                              first_decay_steps=10,\n","                                                              t_mul=1.0,\n","                                                              m_mul=1.0,\n","                                                              alpha=0.0)\n","        lr_callback = tf.keras.callbacks.LearningRateScheduler(annealing, verbose = True)\n","        \n","        history = model.fit(train_dataset,\n","                            epochs=20,\n","                            verbose=1,\n","                            callbacks=[checkpoint, lr_reducer],\n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=valid_dataset)\n","\n","        hist_df = pd.DataFrame(history.history)\n","        hist_df.to_csv(f\"{trained_model_path}/history{i}_2class.csv\")\n","        if ONE_FOLD:\n","            break\n","        "]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2021-07-31T05:10:24.304200Z","iopub.status.busy":"2021-07-31T05:10:24.303672Z","iopub.status.idle":"2021-07-31T05:17:34.721332Z","shell.execute_reply":"2021-07-31T05:17:34.719618Z","shell.execute_reply.started":"2021-07-31T05:10:24.304154Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/effnetV2_model/ (stored 0%)\n","  adding: kaggle/working/effnetV2_model/history1_2class.csv (deflated 58%)\n","  adding: kaggle/working/effnetV2_model/history2_2class.csv (deflated 58%)\n","  adding: kaggle/working/effnetV2_model/model2_2class.h5 (deflated 7%)\n","  adding: kaggle/working/effnetV2_model/model3_2class.h5 (deflated 7%)\n","  adding: kaggle/working/effnetV2_model/model0_2class.h5 (deflated 7%)\n","  adding: kaggle/working/effnetV2_model/history0_2class.csv (deflated 57%)\n","  adding: kaggle/working/effnetV2_model/history3_2class.csv (deflated 58%)\n","  adding: kaggle/working/effnetV2_model/history4_2class.csv (deflated 58%)\n","  adding: kaggle/working/effnetV2_model/model1_2class.h5 (deflated 7%)\n","  adding: kaggle/working/effnetV2_model/model4_2class.h5 (deflated 7%)\n"]}],"source":["!zip -r new_efnV2_cutmix.zip /kaggle/working/effnetV2_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dhgffddfgffkfdfgf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:33:40.042082Z","iopub.status.idle":"2021-07-29T21:33:40.042507Z"},"trusted":true},"outputs":[],"source":["# New Data efnb7 score\n","# vaAUC : 0.913, 0.871, 0.876, 0.900, 0.903\n","# loss : 0.373, 0.414, 0.421, 0.393, 0.368\n","\n","# 왜 두번째 실험이 더 나을까? seed 에 의한 우연? or augmentation (이것 말고는 차이가 딱히 없다.)\n","# -> 데이터 셋이 잘못되었기 때문에 검증 불가\n","# 왜 efnV2 는 성능이 별로인가? -> 잘 모르곘다. 이미지 사이즈가 잘 안맞아서 그런가? 768 로 맞춰줘야 하나?"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:33:40.043442Z","iopub.status.idle":"2021-07-29T21:33:40.043937Z"},"trusted":true},"outputs":[],"source":["if v2:\n","    # train, valid paths for TPU form (just like this)\n","    print(GCS_DS_PATH)\n","    print('GCS_DS_PATH == load_dir\\n')\n","    print((GCS_DS_PATH + '/image/' + df[df['fold'] == 0]['id'] + '.png').head())"]},{"cell_type":"markdown","metadata":{},"source":["# zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:33:40.045129Z","iopub.status.idle":"2021-07-29T21:33:40.04556Z"},"trusted":true},"outputs":[],"source":["# zip\n","#!zip -r image_2class_model.zip /kaggle/working/\n","#!zip -r efnV2_2class_model2.zip /kaggle/working/effnetV2_model\n","#!zip -r new_efnb7_2class.zip /kaggle/working/effnetB7_model\n","!zip -r new_efnv2_xl.zip /kaggle/working/effnetV2_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-21T10:52:55.471358Z","iopub.status.busy":"2021-07-21T10:52:55.470904Z","iopub.status.idle":"2021-07-21T10:52:56.328337Z","shell.execute_reply":"2021-07-21T10:52:56.326943Z","shell.execute_reply.started":"2021-07-21T10:52:55.471315Z"},"trusted":true},"outputs":[],"source":["from glob import glob\n","glob('/kaggle/working/*')\n","!wget -r /kaggle/working/efnV2_2class_model2.zip"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"0ba4c1c78cb01f81f23e4b3fe93b7c67e3b3309ccfffba34bf749370e6b9d870"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","name":"python3"},"language_info":{"name":"python","version":""}},"nbformat":4,"nbformat_minor":4}